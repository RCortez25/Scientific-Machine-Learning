{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGBAVcpcJfuwASz+wAPOGJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/Scientific-Machine-Learning/blob/main/Differential_equations/1DWaveEquation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "Z7Bo6wG-B_Ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code walkthrough"
      ],
      "metadata": {
        "id": "oUqY00_eCA7h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpfoBdEZsM2b"
      },
      "outputs": [],
      "source": [
        "# A\n",
        "using NeuralPDE, Lux, Optimization, OptimizationOptimJL, Plots\n",
        "\n",
        "# B\n",
        "import ModelingToolkit: Interval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **A**: list of the packages to use in this problem\n",
        "    * `NeuralPDE` for defining PDEs symbolically and train PINNs\n",
        "    * `Lux` lightweight NN library to define PINNs architectures\n",
        "    * `Optimization` interface for setting up loss function from `NeuralPDE` and `Lux`\n",
        "    * `OptimizationOptimJL` for plugging `Optim.jl`'s algorithms into the `Optimization` interface.\n",
        "*   **B**: `import A: B` brings only `B` into the scope, in this case, only `Interval` is imported, which is for specifying the domains of the independent variables\n",
        "\n"
      ],
      "metadata": {
        "id": "JpASJYWfs0X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@parameters x t\n",
        "@variables u(..)\n",
        "@derivatives Dt' ~ t\n",
        "@derivatives Dtt'' ~ t\n",
        "@derivatives Dxx'' ~ x"
      ],
      "metadata": {
        "id": "8tn50e5hutG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start setting up the problem.\n",
        "\n",
        "\n",
        "*   `@parameters` for specifying the independent variables, in this case, $x$ and $t$\n",
        "*   `@variables` for specifying the state variable `u(..)`, where `(..)` means that the dependency will be given later\n",
        "*   `@derivatives` defines the differential operators to be used in the problem. Note that it is the same as\n",
        "\n",
        "`Dt = Differential(t)`\n",
        "`Dtt = Differential(t)^2`\n",
        "`Dxx = Differential(x)^2`\n",
        "\n",
        "but this macro is commonly used in NeuralPDE problems."
      ],
      "metadata": {
        "id": "GfhbRm_rvJ3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "boundary_conditions = [\n",
        "    u(0, t) ~ 0,\n",
        "    u(1, t) ~ 0,\n",
        "    u(x, 0) ~ x * (1 - x),\n",
        "    Dt(u(x, 0)) ~ 0\n",
        "]\n",
        "\n",
        "# B\n",
        "domains = [\n",
        "    x ∈ Interval(0.0, 1.0),\n",
        "    t ∈ Interval(0.0, 1.0)\n",
        "]"
      ],
      "metadata": {
        "id": "al9G23J9xp0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**A**: Definition of the initial and boundary conditions. We have\n",
        "* $u(0,t)=0$, at the left boundary $x=0$. This is a Dirichlet BC and together with the next one at $x=1$ indicates that the wave will be fixed at both ends.\n",
        "* $u(1,t)=0$, at the right boundary $x=1$, Dirchlet BC as well.\n",
        "* $u(x,0)=x(1-x)$, the initial displacement of the wave at $t=0$. This is the initial shape of the wave and it's a parabola.\n",
        "* $∂_tu(x,0)=0$, initial velocity at $t=0$, released from rest.\n",
        "\n",
        "\n",
        "**B**: Definition of the rectangular domain $(x,t)\\in[0,1]\\times[0,1]$."
      ],
      "metadata": {
        "id": "aDO_cOvSx3ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "const c = 1.0\n",
        "\n",
        "# B\n",
        "equation = [\n",
        "    Dtt(u(x, t)) ~ (c^2) * Dxx(u(x, t))\n",
        "]"
      ],
      "metadata": {
        "id": "ShMNePxP0K-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A**: Wave speed, in this case $1$ m/s, defined at the global scope with `const` for keeping things stable and fast.\n",
        "\n",
        "**B**: Definition of the wave equation in symbolic form\n",
        "\n",
        "$$\n",
        "∂_{tt}u=c^2\\partial_{xx}u\n",
        "$$"
      ],
      "metadata": {
        "id": "hie8dfZT2WPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "input_dimension = 2 # x and t\n",
        "output_dimension = 1\n",
        "\n",
        "# B\n",
        "layer_0 = Lux.Dense(input_dimension, 16, Lux.tanh)\n",
        "layer_1 = Lux.Dense(16, 16, Lux.tanh)\n",
        "layer_2 = Lux.Dense(16, output_dimension)\n",
        "\n",
        "# C\n",
        "NN = Lux.Chain(\n",
        "    layer_0,\n",
        "    layer_1,\n",
        "    layer_2\n",
        ")"
      ],
      "metadata": {
        "id": "uNvc9IFn3YLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - The input dimension for the neural network will be 2, which is the number of independent variables $x$ and $t$. The output will be the value of the field $u(x,t)$, that is, the network maps\n",
        "\n",
        "$$\\mathbb{R}^2⟶\\mathbb{R}:[x,t]→u(x,t)$$\n",
        "\n",
        "because $u$ is a scalar field.\n",
        "\n",
        "**B** - Creating of the fully connected layers of the NN, in the form `Lux.Dense(in, out, activation)`.\n",
        "*   `layer_0` accepts 2 inputs $(x,y)$, has 16 outputs (this is arbitrary and can be changed), and uses `Lux.tanh` which is a tanh activation function.\n",
        "*   `layer_1` accepts 16 inputs from the previous layer, has 16 outputs (this is arbitrary and can be changed), and uses `Lux.tanh` which is a tanh activation function.\n",
        "*   `layer_2` accepts 16 inputs from the previous layer and has 1 output, the value of the field $u(x,t)$. In this case the layer is linear (no activation function) because one is predicting any real number (no need for tanh to squeeze the numbers, or ReLU, etc, that constraint the ourput number to a certain set).\n",
        "\n",
        "**C** - Assembling the NN with the defined layers. The weights and biases are not created yet, this is just the architecture."
      ],
      "metadata": {
        "id": "u2_rgdMp5Lnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A\n",
        "dx = 0.1\n",
        "\n",
        "# B\n",
        "discretization = PhysicsInformedNN(NN, GridTraining(dx))"
      ],
      "metadata": {
        "id": "A4EGwRRO6wyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Choosing the grid spacing for the domains, in this case the step will then be $0.0, 0.1, 0.2, ..., 1.0$ along each axis $x$ and $t$. Since these correspond to 11 values for each axis, one will have $11\\times11=121$ interior + boundary points. Recall that smaller `dx` implies better resolution (more grid points) but more compute power.\n",
        "**B** - The `PhysicsInformedNN` takes the architecrure and a training strategy (here `GridTraining()`, there are other options) to create a discretization object that will be used for building the loss (PDE residuals + BC/IC residuals).\n",
        "*   `GridTraining(dx)` samples collocation points on the Cartesian grid with spacing `dx` using the BCs and ICs. The interior points enforce PDE residuals while the boundary and initial points enforce the BCs and ICs defined above. Other training strategies like `QuasiRandomTraining()` make use of space-filling random sampling instead of a grid."
      ],
      "metadata": {
        "id": "9-GGcHZNA2xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "@named pde_system = PDESystem(\n",
        "    equation,\n",
        "    boundary_conditions,\n",
        "    domains,\n",
        "    [x, t],\n",
        "    [u(x, t)]\n",
        ")\n",
        "\n",
        "# B\n",
        "problem = discretize(pde_system, discretization)"
      ],
      "metadata": {
        "id": "Zh2dlFHsEKM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Assembling the PDE system withe the `PDESystem` constructor passing it the equation or systems of equations to be solved, the BC/IC, the domains of the independend variables, the independet variables and the dependent variables. `@named` is used to give the system an internal name for bookeeping.\n",
        "\n",
        "**B** - Producing the problem to be solved by appliyng the \"instructions\" of the `discretization` object to the PDE system just created. The object returned is compatible with `Optimization.solve`, and it contains:\n",
        "*   The NN forward pass\n",
        "*   The residuals at all sampled points\n",
        "*   The summed loss ready for an optimizer"
      ],
      "metadata": {
        "id": "g1VdxlKPHGk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "optimizer = OptimizationOptimJL.BFGS()\n",
        "\n",
        "# B\n",
        "callback = function (p, loss)\n",
        "    println(\"Current loss: $loss\")\n",
        "    return false\n",
        "end\n",
        "\n",
        "# C\n",
        "solution = Optimization.solve(problem, optimizer, callback = callback, maxiters = 1000)\n",
        "\n",
        "# D\n",
        "phi = discretization.phi"
      ],
      "metadata": {
        "id": "yol9FUk1IjvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Choosing the quasi-Newton BFGS optimizer. Can also be ADAM or LBFGS. This particular optimizer uses gradient + an internal Hessian approximation.\n",
        "\n",
        "**B** - The *callback* is a function that runs after each optimizer step. The arguments are:\n",
        "*   `p` the current parameter vector, all the weights and biases flattened.\n",
        "*   `loss` current loss value (PDE + IC/BC residuals)\n",
        "\n",
        "In this case one just prints the loss value for tracking. `return false` ensures it keeps running, if set to `true` the optimizer stops early.\n",
        "\n",
        "**C** - This kicks-off training and gives the solution of the training, containing\n",
        "*   `solution.minimizer`, which stores the trained parameter vector, that is, the trained weights.\n",
        "*   `solution.minimum`, the final loss value\n",
        "*   `solution.retcode` termination reason, e.g., `Success`\n",
        "\n",
        "**D** - This is the model evaluator of the discretization, this is uded for evaluating the field on points on the grid or a other points for plotting and validationg, that is, `phi` is $\\hat{u}(x,t)$ for any given point $(x,t)$. In the case of PINNs, this needs the input point and the trainable parameters, that is, it is more like $\\hat{u}(x,t,\\theta)$ for the vector of parameters $\\theta$."
      ],
      "metadata": {
        "id": "7bcs93fA8NMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u_exact(x, t; K=100) = begin\n",
        "    s = 0.0\n",
        "    for k in 0:K-1\n",
        "        n = 2k + 1\n",
        "        coefficient = 8.0 / ((pi*n)^3)\n",
        "        term = coefficient * cos(n*pi*c*t) * sin(n*pi*x)\n",
        "        s += term\n",
        "    end\n",
        "    s\n",
        "end"
      ],
      "metadata": {
        "id": "2gw38EDwAWaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This generates points for the analytic solution. In this case, it is given by\n",
        "\n",
        "$$\n",
        "u(x,t)=\\sum_{n=1,odd}^{\\infty}{\\frac{8}{(n\\pi)^3}\\cos(n\\pi ct)\\sin(n\\pi x)}\n",
        "$$\n",
        "\n",
        "So this loop takes in $x$ and $t$ values and starts iterating for 100 steps to approximate the value of the field $u$ at that particular point. This is necessary for plotting againts the predicted values."
      ],
      "metadata": {
        "id": "O6T1qF-HbewS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A\n",
        "θ = solution.minimizer\n",
        "\n",
        "# B\n",
        "x_dom, t_dom = domains\n",
        "x_points = infimum(x_dom.domain):(dx/10):supremum(x_dom.domain)\n",
        "t_points = infimum(t_dom.domain):(dx/10):supremum(t_dom.domain)\n",
        "\n",
        "# C\n",
        "u_predicted = [first(phi([x, t], θ)) for x in x_points, t in t_points]\n",
        "\n",
        "# D\n",
        "u_real = [u_exact(x, t; K=200) for x in x_points, t in t_points]\n",
        "\n",
        "# E\n",
        "difference_of_u = @.abs(u_predicted - u_real)"
      ],
      "metadata": {
        "id": "pccqwM9bcwC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Retrieving the learned weights to use with the evaluator `phi`\n",
        "\n",
        "**B** - Generate the range of values for the two domians, for plotting.\n",
        "*   `x_dom, t_dom = domains` unpacks the domains as define above\n",
        "*   `infimum(x_dom.domain)` retrieves the lowest value of the $x$ domain, using the `domain` method\n",
        "*   `dx/10` defines the step-size, in this case, $0.1/10=0.01$\n",
        "*   `supremum(x_dom.domain`) retrieves the max value of the domain\n",
        "\n",
        "The syntax is then `initial_point:step_size:final_point`. It is worth noting that in this case we already know the min and max values, so we could also write\n",
        "\n",
        "```\n",
        "x_points = 0.0:(dx/10):1.0\n",
        "t_points = 0.0:(dx/10):1.0\n",
        "```\n",
        "\n",
        "but the form used in the code helps when one doesn't know those values.\n",
        "\n",
        "**C** - Loop over every point $(x,t)$ of the evaluation grid `x_points, t_points`, creating a matrix with `x=rows, t=columns`. At every iteration, that is, at every point on the grid, the evaluator is called with the point in question and the parameters `phi([point], parameters)=phi([x,t], theta)`. Then `first(...)` grabs the first element of whatever `phi` returns, for example, if it returns `SVector(0.123)` we want the value $0.123$ and not the whole container `SVector(...)`, the same with `[0.123]`, we want the value $0.123$, not the container itself `[...]`.\n",
        "\n",
        "**D** - Evaluation of the analytical solution on the same grid as the predicted field. Increasing `K` until the series visually converges.\n",
        "\n",
        "**E** - Elementwise operation for taking the absolute value of the difference between real and predicted values. This will also help for visualizing the differences between the two. The macro `@.` makes the operation elementwise."
      ],
      "metadata": {
        "id": "Cbp8Aj3oc9TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots"
      ],
      "metadata": {
        "id": "kajOyDl1CHEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "cl = (minimum(u_real), maximum(u_real))\n",
        "\n",
        "# B\n",
        "p1 = plot(x_points, t_points, u_real;\n",
        "          linetype=:contourf, title=\"Analytic\",\n",
        "          xlabel=\"x\", ylabel=\"t\", colorbar=true, clims=cl)\n",
        "\n",
        "# C\n",
        "p2 = plot(x_points, t_points, u_predicted;\n",
        "          linetype=:contourf, title=\"Predicted\",\n",
        "          xlabel=\"x\", ylabel=\"t\", colorbar=true, clims=cl)\n",
        "\n",
        "# D\n",
        "p3 = plot(x_points, t_points, difference_of_u;\n",
        "          linetype=:contourf, title=\"Error\",\n",
        "          xlabel=\"x\", ylabel=\"t\", colorbar=true,\n",
        "          clims=(0, maximum(difference_of_u)))\n",
        "\n",
        "# E\n",
        "plot(p1, p2, p3; layout=(1,3), link=:both, size=(1200, 380))"
      ],
      "metadata": {
        "id": "-3-J7h2tfWSB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Retrieving the min and max values of the analytic solution fo force plots to be on that scale, so that we can compare them on the same scale.\n",
        "\n",
        "**B** - Plotting the analytic solution. `clims` stands for _color limits_, and in this case we use the scale obtained with the min and max values of the analytic solution.\n",
        "\n",
        "**C** - Same for the PINN predicted solution. Here we use the same solor limits as the analytic solution for comparing the both.\n",
        "\n",
        "**D** - Plot of the difference between analytic and the predicted solution, that is, the error. In this case, we force the scale to start at 0 up to the max value of the error (there are no values $<0$ because those were calculated using the absolute value).\n",
        "\n",
        "**E** - Plotting the 3 plots in 1 row and 3 columns. `link=:both` keeps $x$ and $t$ axes synced for the 3 plots when zooming."
      ],
      "metadata": {
        "id": "73WR5dxLMRFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Model Diagram](Images/1DWave.png)"
      ],
      "metadata": {
        "id": "XxsqYfKGLIXw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BIXESPvcLI38"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}