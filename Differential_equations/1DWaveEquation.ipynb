{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyTTTzCjCtvEjSh+5hcbvs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/Scientific-Machine-Learning/blob/main/Differential_equations/1DWaveEquation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpfoBdEZsM2b"
      },
      "outputs": [],
      "source": [
        "# A\n",
        "using NeuralPDE, Lux, Optimization, OptimizationOptimJL\n",
        "\n",
        "# B\n",
        "import ModelingToolkit: Interval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **A**: list of the packages to use in this problem\n",
        "    * `NeuralPDE` for defining PDEs symbolically and train PINNs\n",
        "    * `Lux` lightweight NN library to define PINNs architectures\n",
        "    * `Optimization` interface for setting up loss function from `NeuralPDE` and `Lux`\n",
        "    * `OptimizationOptimJL` for plugging `Optim.jl`'s algorithms into the `Optimization` interface.\n",
        "*   **B**: `import A: B` brings only `B` into the scope, in this case, only `Interval` is imported, which is for specifying the domains of the independent variables\n",
        "\n"
      ],
      "metadata": {
        "id": "JpASJYWfs0X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@parameters x t\n",
        "@variables u(..)\n",
        "@derivatives Dt' ~ t\n",
        "@derivatives Dtt'' ~ t\n",
        "@derivatives Dxx'' ~ x"
      ],
      "metadata": {
        "id": "8tn50e5hutG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start setting up the problem.\n",
        "\n",
        "\n",
        "*   `@parameters` for specifying the independent variables, in this case, $x$ and $t$\n",
        "*   `@variables` for specifying the state variable `u(..)`, where `(..)` means that the dependency will be given later\n",
        "*   `@derivatives` defines the differential operators to be used in the problem. Note that it is the same as\n",
        "\n",
        "`Dt = Differential(t)`\n",
        "`Dtt = Differential(t)^2`\n",
        "`Dxx = Differential(x)^2`\n",
        "\n",
        "but this macro is commonly used in NeuralPDE problems."
      ],
      "metadata": {
        "id": "GfhbRm_rvJ3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "boundary_conditions = [\n",
        "    u(0, t) ~ 0,\n",
        "    u(1, t) ~ 0,\n",
        "    u(x, 0) ~ x * (1 - x),\n",
        "    Dt(u(x, 0)) ~ 0\n",
        "]\n",
        "\n",
        "# B\n",
        "domains = [\n",
        "    x ∈ Interval(0.0, 1.0),\n",
        "    t ∈ Interval(0.0, 1.0)\n",
        "]"
      ],
      "metadata": {
        "id": "al9G23J9xp0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**A**: Definition of the initial and boundary conditions. We have\n",
        "* $u(0,t)=0$, at the left boundary $x=0$. This is a Dirichlet BC and together with the next one at $x=1$ indicates that the wave will be fixed at both ends.\n",
        "* $u(1,t)=0$, at the right boundary $x=1$, Dirchlet BC as well.\n",
        "* $u(x,0)=x(1-x)$, the initial displacement of the wave at $t=0$. This is the initial shape of the wave and it's a parabola.\n",
        "* $∂_tu(x,0)=0$, initial velocity at $t=0$, released from rest.\n",
        "\n",
        "\n",
        "**B**: Definition of the rectangular domain $(x,t)\\in[0,1]\\times[0,1]$."
      ],
      "metadata": {
        "id": "aDO_cOvSx3ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "const c = 1.0\n",
        "\n",
        "# B\n",
        "equation = [\n",
        "    Dtt(u(x, t)) ~ (c^2) * Dxx(u(x, t))\n",
        "]"
      ],
      "metadata": {
        "id": "ShMNePxP0K-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A**: Wave speed, in this case $1$ m/s, defined at the global scope with `const` for keeping things stable and fast.\n",
        "\n",
        "**B**: Definition of the wave equation in symbolic form\n",
        "\n",
        "$$\n",
        "∂_{tt}u=c^2\\partial_{xx}u\n",
        "$$"
      ],
      "metadata": {
        "id": "hie8dfZT2WPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "input_dimension = 2 # x and t\n",
        "output_dimension = 1\n",
        "\n",
        "# B\n",
        "layer_0 = Lux.Dense(input_dimension, 16, Lux.tanh)\n",
        "layer_1 = Lux.Dense(16, 16, Lux.tanh)\n",
        "layer_2 = Lux.Dense(16, output_dimension)\n",
        "\n",
        "# C\n",
        "NN = Lux.Chain(\n",
        "    layer_0,\n",
        "    layer_1,\n",
        "    layer_2\n",
        ")"
      ],
      "metadata": {
        "id": "uNvc9IFn3YLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - The input dimension for the neural network will be 2, which is the number of independent variables $x$ and $t$. The output will be the value of the field $u(x,t)$, that is, the network maps\n",
        "\n",
        "$$\\mathbb{R}^2⟶\\mathbb{R}:[x,t]→u(x,t)$$\n",
        "\n",
        "because $u$ is a scalar field.\n",
        "\n",
        "**B** - Creating of the fully connected layers of the NN, in the form `Lux.Dense(in, out, activation)`.\n",
        "*   `layer_0` accepts 2 inputs $(x,y)$, has 16 outputs (this is arbitrary and can be changed), and uses `Lux.tanh` which is a tanh activation function.\n",
        "*   `layer_1` accepts 16 inputs from the previous layer, has 16 outputs (this is arbitrary and can be changed), and uses `Lux.tanh` which is a tanh activation function.\n",
        "*   `layer_2` accepts 16 inputs from the previous layer and has 1 output, the value of the field $u(x,t)$. In this case the layer is linear (no activation function) because one is predicting any real number (no need for tanh to squeeze the numbers, or ReLU, etc, that constraint the ourput number to a certain set)."
      ],
      "metadata": {
        "id": "u2_rgdMp5Lnd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A4EGwRRO6wyB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}