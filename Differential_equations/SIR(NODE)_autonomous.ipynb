{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORsOjHJBfmjQKjb4QL2wmt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/Scientific-Machine-Learning/blob/main/Differential_equations/SIR(NODE)_autonomous.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "n9BN5IlSTpl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code walkthrough"
      ],
      "metadata": {
        "id": "KXhVIOaMTrlj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9JZI3s8TF8e"
      },
      "outputs": [],
      "source": [
        "# A\n",
        "using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimJL,\n",
        "    OptimizationOptimisers, Random, Plots, ModelingToolkit\n",
        "\n",
        "# B\n",
        "random_number_generator = Random.default_rng()\n",
        "Random.seed!(rng, 42)\n",
        "\n",
        "# C\n",
        "#------ Generate ground-truth data\n",
        "# Autonomous system where t is the independent variable\n",
        "@parameters β γ N\n",
        "@independent_variables t\n",
        "@variables S(t) I(t) R(t)\n",
        "Dt = Differential(t)\n",
        "\n",
        "eqs = [\n",
        "    Dt(S) ~ -(β*S*I)/N,\n",
        "    Dt(I) ~ ((β*S*I)/N) - γ*I,\n",
        "    Dt(R) ~ γ*I\n",
        "]\n",
        "\n",
        "@named system = ODESystem(eqs, t, [S, I, R], [β, γ, N])\n",
        "simplified = structural_simplify(system)\n",
        "\n",
        "parameter_map = Dict(β => 0.3, γ => 0.1, N => 1000)\n",
        "initial_conditions = Dict(I => 1, R => 0, S => 1000 - 1 - 0)\n",
        "timespan = (0.0, 160.0) # in days\n",
        "\n",
        "problem = ODEProblem(simplified,\n",
        "                     merge(initial_conditions, parameter_map),\n",
        "                     timespan)\n",
        "\n",
        "solution = solve(problem, Tsit5(), saveat=1)\n",
        "ground_truth = Array(solution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Importing the packages\n",
        "*   `ComponentArrays` is for packaging parameters into a single vector with names and structure\n",
        "*   `DiffEqFlux` for using `NeuralODE`\n",
        "*   `OptimizationOptimisers` adapter for `Optimisers.jl`, for using ADAM.\n",
        "*   `Random` for seeding random number generators for reproducibility\n",
        "\n",
        "**B** - Create a seeded random number generator for reproducibility\n",
        "\n",
        "**C** - The rest of the code is used for solving the ODE system and generate ground-truth data for comparing with the NN results. `ground_truth` stores the results of the integrator. Note that `saveat=1` means that we're saving for each day, 1 day at a time."
      ],
      "metadata": {
        "id": "3APQNlwzhY4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "const input_dimension = 3 # S, I, R\n",
        "const output_dimension = 3 # Derivatives of S, I, R\n",
        "\n",
        "# B\n",
        "layer_0 = Lux.Dense(input_dimension, 32, Lux.tanh)\n",
        "layer_1 = Lux.Dense(32, 32, Lux.tanh)\n",
        "layer_2 = Lux.Dense(32, output_dimension)\n",
        "\n",
        "# C\n",
        "NN = Lux.Chain(\n",
        "    layer_0,\n",
        "    layer_1,\n",
        "    layer_2\n",
        ")\n",
        "\n",
        "# D\n",
        "NN64 = Lux.f64(NN)\n",
        "\n",
        "# E\n",
        "parameters, state = Lux.setup(random_number_generator, NN64)"
      ],
      "metadata": {
        "id": "toe20fhAixJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Defining the input and output dimensions. `const` locks them up in order to avoid accidental re-writing of the dimensions. In the case of NeuralODEs, as one is predicting the derivative, the inputs are $t,S,I,R$ in that order, and the output numbers represent the value of their derivatives $\\dot{S},\\dot{I},\\dot{R}$.\n",
        "\n",
        "**B** - Definition of the NN architecture, in this case, 3 layers:\n",
        "*   `layer_0` is the input layer, recieves 4 inputs and outputs 32 numbers (this is arbitrary and can be changed) using `tanh` activation function.\n",
        "*   `layer_1` First hidden layer with 32 inputs (from the previous layer) and 32 outputs using `tanh` activation layer.\n",
        "*   `layer_2` is the output layer. It recieves 32 inputs (from the previous layer) and outputs 3 numbers, namely, the derivatives as stated before.\n",
        "\n",
        "**C** - Creating of the NN using the defined layers\n",
        "\n",
        "**D** - Makes the NN use `Float64` for better performance with the solvers\n",
        "\n",
        "**E** - Initializing the network. It returns two objects:\n",
        "*   `parameters` which is the set of all trainable parameters and biases to be optimized later during training.\n",
        "*   `state` all non-trainable internal states some layers keep (BatchNorm running means, etc)."
      ],
      "metadata": {
        "id": "Aq8OZgD_n8E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "neural_ODE_problem = NeuralODE(NN64, timespan, Tsit5(); saveat = 1)\n",
        "\n",
        "# B\n",
        "function neural_ode_predictions(parameters, state)\n",
        "    u0 = Float64[\n",
        "        initial_conditions[S],\n",
        "        initial_conditions[I],\n",
        "        initial_conditions[R],\n",
        "    ]\n",
        "    trajectory, new_state = neural_ODE_problem(u0, parameters, state)\n",
        "    return Array(trajectory)\n",
        "end"
      ],
      "metadata": {
        "id": "1nmfXPkTqF2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Builds the solver layer. It prepares everything so that when called, an ODE whose RHS is the neural network passed to it, is integrated. It\n",
        "\n",
        "*   Indicates that the ODE's RHS (the ODE rule) is the neural network passed to it.\n",
        "*   Stores how to solve it: the time span, the algorithm, at what time steps to solve it, and tolerances like `reltol`, `abstol`, not used in this example.\n",
        "\n",
        "This is just the constructor, where everything is configured, but not run yet. The returned object is called as `neural_ODE_problem(initial_conditions, parameters, state)`.\n",
        "\n",
        "**B** - Function for a NeuralODE forward pass. It accepts the trainable parameters of the NN and the state (BatchNorms, etc., in this case, the system is stateless, that is, just Dense + tanh).\n",
        "\n",
        "*   `u0` is the array of initial conditions. Earlier, we defined these using a dictionary, but we need to unpack them into a vector.\n",
        "*   The `NeuralODE` is run, passing it the initial conditions, trainable parameters and the state. This integrates the ODE with the NN as the RHS using the solver, time span, etc, as defined above. This returns two objects:\n",
        "    *   `trajectory`: The trajectory matrix, in this case, of shape roughly `(3,161)`, that is, 3 rows (for each S, I, and R), and 161 columns for each day the simulation was instructed to run (`saveat=1.0`).\n",
        "    * `new_state`: The updated state (BatchNorms, etc), but in this case, since the system is stateless, there's no need for the use of this variable.\n",
        "\n",
        "Changing `parameters` amounts to changing the NN weights, and this changes the predicted values and the integrated trajectory.\n",
        "\n",
        "In the end, the function returns the `trajectory` as a Julia `Array`, ignoring `state` in this particular case."
      ],
      "metadata": {
        "id": "Y--0Jk8Bjs7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "function loss_neuralode(parameters, state)\n",
        "    predicted_trajectory = neural_ode_predictions(parameters, state)\n",
        "    loss = sum(abs2, ground_truth .- predicted_trajectory)\n",
        "    return loss, predicted_trajectory\n",
        "end"
      ],
      "metadata": {
        "id": "rVcsVM5GyCcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "1.  The `NeuralODE` constructor expects a function as one of its parameters. Specifically, a function of the form\n",
        "\n",
        "$$f(u,p,t)⟶du/dt$$\n",
        "\n",
        "(though one often only writes $f(u,p,t)⟶du$) because the solver, who lives inside `NeuralODE` (for example, `Tsit5()`), integrates that $du/dt$ and gives $u=[S,I,R]$. That is, the output of `NeuralODE` is the state `u` that can be called with initial conditions in order to obtain a whole trajectory."
      ],
      "metadata": {
        "id": "xw4j9L6-NS4n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pFacZtulWY0l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}