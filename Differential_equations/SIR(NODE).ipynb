{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuzCm3tMtW/GhLAgumqhQ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/Scientific-Machine-Learning/blob/main/Differential_equations/SIR(NODE).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "n9BN5IlSTpl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code walkthrough"
      ],
      "metadata": {
        "id": "KXhVIOaMTrlj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9JZI3s8TF8e"
      },
      "outputs": [],
      "source": [
        "# A\n",
        "using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimJL,\n",
        "    OptimizationOptimisers, Random, Plots, ModelingToolkit\n",
        "\n",
        "# B\n",
        "random_number_generator = Random.default_rng()\n",
        "Random.seed!(rng, 42)\n",
        "\n",
        "# C\n",
        "#------ Generate ground-truth data\n",
        "@parameters t β γ N\n",
        "@variables S(t) I(t) R(t)\n",
        "Dt = Differential(t)\n",
        "\n",
        "eqs = [\n",
        "    Dt(S) ~ -(β*S*I)/N,\n",
        "    Dt(I) ~ ((β*S*I)/N) - γ*I,\n",
        "    Dt(R) ~ γ*I\n",
        "]\n",
        "\n",
        "@named system = ODESystem(eqs, t, [S, I, R], [β, γ, N])\n",
        "simplified = structural_simplify(system)\n",
        "\n",
        "parameter_map = Dict(β => 0.3, γ => 0.1, N => 1000)\n",
        "initial_conditions = Dict(I => 1, R => 0, S => 1000 - 1 - 0)\n",
        "timespan = (0.0, 160.0) # in days\n",
        "\n",
        "problem = ODEProblem(simplified,\n",
        "                     merge(initial_conditions, parameter_map),\n",
        "                     timespan)\n",
        "\n",
        "solution = solve(problem, Tsit5(), saveat=0.5)\n",
        "ground_truth = Array(solution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Importing the packages\n",
        "*   `ComponentArrays` is for packaging parameters into a single vector with names and structure\n",
        "*   `DiffEqFlux` for using `NeuralODE`\n",
        "*   `OptimizationOptimisers` adapter for `Optimisers.jl`, for using ADAM.\n",
        "*   `Random` for seeding random number generators for reproducibility\n",
        "\n",
        "**B** - Create a seeded random number generator for reproducibility\n",
        "\n",
        "**C** - The rest of the code is used for solving the ODE system and generate ground-truth data for comparing with the NN results. `ground_truth` stores the results of the integrator."
      ],
      "metadata": {
        "id": "3APQNlwzhY4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "const input_dimension = 4 # t, S, I, R\n",
        "const output_dimension = 3 # Derivatives of S, I, R\n",
        "\n",
        "# B\n",
        "layer_0 = Lux.Dense(input_dimension, 32, Lux.tanh)\n",
        "layer_1 = Lux.Dense(32, 32, Lux.tanh)\n",
        "layer_2 = Lux.Dense(32, output_dimension)\n",
        "\n",
        "# C\n",
        "NN = Lux.Chain(\n",
        "    layer_0,\n",
        "    layer_1,\n",
        "    layer_2\n",
        ")\n",
        "\n",
        "# D\n",
        "parameters, state = Lux.setup(random_number_generator, NN)\n",
        "\n",
        "# E\n",
        "neural_ODE_problem = NeuralODE(NN, timespan, Tsit5(); saveat = 0.1)"
      ],
      "metadata": {
        "id": "toe20fhAixJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - Defining the input and output dimensions. `const` locks them up in order to avoid accidental re-writing of the dimensions. In the case of NeuralODEs, as one is predicting the derivative, the inputs are $t,S,I,R$ in that order, and the output numbers represent the value of their derivatives $\\dot{S},\\dot{I},\\dot{R}$.\n",
        "\n",
        "**B** - Definition of the NN architecture, in this case, 3 layers:\n",
        "*   `layer_0` is the input layer, recieves 4 inputs and outputs 32 numbers (this is arbitrary and can be changed) using `tanh` activation function.\n",
        "*   `layer_1` First hidden layer with 32 inputs (from the previous layer) and 32 outputs using `tanh` activation layer.\n",
        "*   `layer_2` is the output layer. It recieves 32 inputs (from the previous layer) and outputs 3 numbers, namely, the derivatives as stated before.\n",
        "\n",
        "**C** - Creating of the NN using the defined layers\n",
        "\n",
        "**D** - Initializing the network. It returns two objects:\n",
        "*   `parameters` which is the set of all trainable parameters and biases to be optimized later during training.\n",
        "*   `state` all non-trainable internal states some layers keep (BatchNorm running means, etc)."
      ],
      "metadata": {
        "id": "Aq8OZgD_n8E1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1nmfXPkTqF2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}