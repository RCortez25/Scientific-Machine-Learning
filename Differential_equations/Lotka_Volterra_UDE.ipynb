{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaanNqL+bxBPSCOPt/iHai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCortez25/Scientific-Machine-Learning/blob/main/Differential_equations/Lotka_Volterra_UDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "---"
      ],
      "metadata": {
        "id": "l9VH1qEKG71Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code walkthrough\n",
        "---"
      ],
      "metadata": {
        "id": "UJtmHDolIrLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ground truth\n",
        "---"
      ],
      "metadata": {
        "id": "dNIffpWKIveF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using JLD, Lux, DifferentialEquations, Optimization, OptimizationOptimJL\n",
        "using Random, Plots, ComponentArrays, ModelingToolkit\n",
        "\n",
        "random_number_generator = Random.default_rng()\n",
        "Random.seed!(random_number_generator, 42)\n",
        "\n",
        "#------ Generate ground-truth data\n",
        "@parameters α β δ γ\n",
        "@independent_variables t\n",
        "@variables x(t) y(t)\n",
        "Dt = Differential(t)\n",
        "\n",
        "eqs = [\n",
        "    Dt(x) ~ α*x - β*x*y,\n",
        "    Dt(y) ~ -δ*y + γ*x*y\n",
        "]\n",
        "\n",
        "@named system = ODESystem(eqs, t, [x, y], [α, β, δ, γ])\n",
        "simplified = structural_simplify(system)\n",
        "\n",
        "N_days = 25\n",
        "\n",
        "parameter_map = Dict(α => 1.0, β => 0.02, δ => 0.5, γ => 0.02)\n",
        "initial_conditions = Dict(x => 20, y => 10)\n",
        "timespan = (0.0, N_days)"
      ],
      "metadata": {
        "id": "7O1EMhMKHADm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is for generating ground truth data with the real Lotka-Volterra system of equations. Note that in this case que values of the parameters\n",
        "\n",
        "`α => 1.0, β => 0.02, δ => 0.5, γ => 0.02`\n",
        "\n",
        "were selected rather pedagogically, in order to have a \"nice\" system."
      ],
      "metadata": {
        "id": "2JmqiZzlHjsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "problem = ODEProblem(\n",
        "    simplified,\n",
        "    merge(initial_conditions, parameter_map),\n",
        "    timespan\n",
        ")\n",
        "\n",
        "solution = solve(problem; saveat = 0.1)\n",
        "solution = Array(solution) # solution[1,:] = y, solution[2,:] = x\n",
        "\n",
        "x_ground_truth = solution[2, :]\n",
        "y_ground_truth = solution[1, :]"
      ],
      "metadata": {
        "id": "UyYej9QqH27V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the ODE problem is generated as `problem`. The problem is solved with default parameters (not a specific integrator).\n",
        "\n",
        "The important thing to note, is that, for some reason, `structural_simplify` rearranges variables, so that instead of having `(x,y)` one has `(y,x)`. That's why, in order to retrieve the `x` values, we have to use the second index as in\n",
        "\n",
        "`x_ground_truth = solution[2, :]`\n",
        "\n",
        "and the first index for `y`\n",
        "\n",
        "`y_ground_truth = solution[1, :]`"
      ],
      "metadata": {
        "id": "c8_jiCiyH7hW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Universal Differential Equation\n",
        "---"
      ],
      "metadata": {
        "id": "k6v8ZW_wI3bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A\n",
        "NN1 = Lux.Chain(\n",
        "    Lux.Dense(2, 32, relu),\n",
        "    Lux.Dense(32, 1, softplus)\n",
        ")\n",
        "parameters_1, state_1 = Lux.setup(random_number_generator, NN1)\n",
        "\n",
        "NN2 = Lux.Chain(\n",
        "    Lux.Dense(2, 32, relu),\n",
        "    Lux.Dense(32, 1, softplus)\n",
        ")\n",
        "parameters_2, state_2 = Lux.setup(random_number_generator, NN2)\n",
        "\n",
        "# B\n",
        "initial_parameters = (layer_1 = parameters_1, layer_2 = parameters_2)\n",
        "initial_parameters = ComponentArray(initial_parameters)"
      ],
      "metadata": {
        "id": "Slyv9q6rIdnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** - In this case, we're modeling the terms $\\beta xy$ and $\\gamma xy$ of the LV system. That's why, we're creating 2 neural networks here, `NN1` and `NN2`, in order to replace these two terms.\n",
        "\n",
        "Each NN will take 2 inputs, $(x,y)$ and will output 1 number, their corresponding $\\beta xy$ and $\\gamma xy$. Note also that in the output layer, the `sofplus` activation is used in order to guarantee positive numbers, as we want the interaction terms to be positive. The softplus funcion is defined as\n",
        "\n",
        "$$x=\\ln(1+e^x)$$\n",
        "\n",
        "That is, it maps negative numbers to numbers very close to 0.\n",
        "\n",
        "We initialize the NNs with `Lux.setup`, then we extract the initial (random) weights and biases with `parameters_1` and `parameters_2`.\n",
        "\n",
        "Each network has\n",
        "\n",
        "*   Dense(2→32): weights 32×2=64, bias 32. Total: 96 parameters\n",
        "*   Dense(32→1): weights 1×32=32, bias 1. Total: 33 parameters\n",
        "\n",
        "that is, 129 trainable parameters per network, for a total of 129×2=258 trainable parameters in total.\n",
        "\n",
        "**B** - We store the initial parameters into a single vector `initial_parameters`, that becomes a flat vector due to `ComponentArrays` of length 258. This vector has named subfields `layer_1` and `layer_2` that contain information on each network's parameters.\n",
        "\n",
        "We will use this vector when solving the UDE, and for training and optimizing as the starting point."
      ],
      "metadata": {
        "id": "j-lQHeaLJDr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function derivative_predictions(du, u, p, t)\n",
        "    # A\n",
        "    (x, y) = u\n",
        "\n",
        "    # B\n",
        "    output1, updated_state1 = Lux.apply(NN1, Float32([x,y]), p.p1, state_1)\n",
        "    output2, updated_state2 = Lux.apply(NN2, Float32([x,y]), p.p2, state_2)\n",
        "\n",
        "    # C\n",
        "    beta_xy = only(output1)\n",
        "    gamma_xy = only(output2)\n",
        "\n",
        "    # D\n",
        "    du[1] = p.α * x - beta_xy\n",
        "    du[2] = -p.δ * y + gamma_xy\n",
        "end"
      ],
      "metadata": {
        "id": "6_dX3JgqJTr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the function to calculate the derivatives of the LV system, that is, the whole RHS of\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\dot{x} &=αx-\\beta xy \\\\\n",
        "\\dot{y} &=-\\delta y+\\gamma xy\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "This is an operation defined **in-place**, meaning that it will write `du` given the current state `u`, parameters `p`, and time `t`.\n",
        "\n",
        "**A** - This splits the current state vector `u` into $x$ and $y$.\n",
        "\n",
        "**B** - Runnning a forward pass of the NNs defined above on the current state $(x,y)$, using parameters the portion `p1` of the parameters vector `p` for NN1, and `p2` for NN2. This returns two objects\n",
        "\n",
        "*   `output1`: the output of the NN forward pass, a scalar\n",
        "*   `updated_state1`: the updated state of BatchNorm, etc. In this example, we don't use this.\n",
        "\n",
        "The same applies for NN2.\n",
        "\n",
        "Note that we perform a transformation to `Float32.` to match the NNs' weights datatype, which is `Float32` as well.\n",
        "\n",
        "**C** - Extract the only element of the vector `output1` and `output2`. These are single-element vectors, e.g. `output1=[2]`, so selecting the only element gives back the scalar `only(output1)=2`.\n",
        "\n",
        "Note that `output1` corresponds to the term $\\beta xy$ whereas `output2` corresponds to $\\gamma xy$.\n",
        "\n",
        "**D** -"
      ],
      "metadata": {
        "id": "AAInIynsOphC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e94kjbdcTv32"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}